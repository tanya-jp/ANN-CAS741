\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{paralist}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Project Title: System Verification and Validation Plan for ANN} 
\author{Tanya Djavaherpour}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Feb. 15, 2024 & 1.0 & Initial Draft\\
Mar. 04, 2024 & 1.1 & Modification According to the Feedback 1\\
\bottomrule
\end{tabularx}

~\\
% \wss{The intention of the VnV plan is to increase confidence in the software.
% However, this does not mean listing every verification and validation technique
% that has ever been devised.  The VnV plan should also be a \textbf{feasible}
% plan. Execution of the plan should be possible with the time and team available.
% If the full plan cannot be completed during the time available, it can either be
% modified to ``fake it'', or a better solution is to add a section describing
% what work has been completed and what work is still planned for the future.}

% \wss{The VnV plan is typically started after the requirements stage, but before
% the design stage.  This means that the sections related to unit testing cannot
% initially be completed.  The sections will be filled in after the design stage
% is complete.  the final version of the VnV plan should have all sections filled
% in.}

\newpage

\tableofcontents

\listoftables
% \wss{Remove this section if it isn't needed}

% \listoffigures
% \wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  ANN & Artificial Neural Network\\
  IM & Instance Model\\
  MG & Module Guide\\
  MIS & Module Interface Specification\\
  N & No \\
  SRS & Software Requirements Specification\\
  T & Test\\
  VnV & Verification and Validation\\
  Y & Yes\\

  \bottomrule
\end{tabular}\\

For complete symbols used within the system, please refer the section 1 in 
SRS \cite{SRS} document.

% \wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
%   \citep{SRS} tables, if appropriate}

% \wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document outlines the Verification and Validation (VnV) plan for the Artificial 
Neural Network for Image Classification project, as detailed in the 
SRS \cite{SRS}. 
The purpose of this VnV plan is to ensure that all requirements and objectives outlined 
in the SRS \cite{SRS} 
are met with accuracy and efficiency.

The organization of this document starts with the General Information about the ANN in \autoref{GeneralInformation}. 
A verification plan is provided in \autoref{Plan} and \autoref{SystemTest} 
describes the system tests, including tests for functional and nonfunctional requirements. 
Test Description is explained in section 5 (will be added).
% \autoref{UnitTest}.

\section{General Information}\label{GeneralInformation}

\subsection{Summary}

The software being validated in this plan is an Artificial Neural Network designed for 
Image Classification, tailored specifically to work with the CIFAR-10 dataset \cite{CIFAR10}. 
It allows users to upload images and efficiently classifies them into predefined 
categories. It operates within the constraints of available computational 
resources and is limited to handling images that fall under the CIFAR-10 dataset \cite{CIFAR10}
categories, ensuring focused and optimized performance in its designated area.

\subsection{Objectives}

The primary objective of this VnV plan is to build confidence in the correctness and 
reliability of the Artificial Neural Network for Image Classification. Our goal is to 
demonstrate that the system can classify images with a high degree of accuracy. We aim 
to significantly improve upon the less than 50\% accuracy achieved in 
previous implementation \cite{ANN:CIFAR10}, 
acknowledging that reaching 100\% accuracy is not feasible due to inherent limitations in 
ANN models and the variability of image data. The focus will be on achieving the highest 
possible accuracy within these constraints. The system's accuracy will be measured through 
defined quantitative methods such as the cost function in SRS \cite{SRS}.

% \wss{State what is intended to be accomplished.  The objective will be around
%   the qualities that are most important for your project.  You might have
%   something like: ``build confidence in the software correctness,''
%   ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
%   just those that are most important.}

% \wss{You should also list the objectives that are out of scope.  You don't have 
% the resources to do everything, so what will you be leaving out.  For instance, 
% if you are not going to verify the quality of usability, state this.  It is also 
% worthwhile to justify why the objectives are left out.}

% \wss{The objectives are important because they highlight that you are aware of 
% limitations in your resources for verification and validation.  You can't do everything, 
% so what are you going to prioritize?  As an example, if your system depends on an 
% external library, you can explicitly state that you will assume that external library 
% has already been verified by its implementation team.}

\subsection{Relevant Documentation}

The ANN project is supported by several crucial documents. 
These include a
Problem Statement \cite{ProblemStatement}, which introduces the initial concept, and a 
Software Requirements Specification \cite{SRS} that outlines the necessary system requirements, accompanied 
by a Verification and Validation Report \cite{VnVReport} to ensure the system's compliance and efficacy.

% \wss{Reference relevant documentation.  This will definitely include your SRS
%   and your other project documents (design documents, like MG, MIS, etc).  You
%   can include these even before they are written, since by the time the project
%   is done, they will be written.}

% \citet{SRS}

% \wss{Don't just list the other documents.  You should explain why they are relevant and 
% how they relate to your VnV efforts.}

\section{Plan}\label{Plan}

In this section, the VnV plan of ANN is described. 
It begins with an introduction to the verification and validation team (\autoref{VnVTeam}) and 
introduces the members and thei rules. 
Then is followed by the SRS verification plan (\autoref{SRSVerPlan}), 
design verification plan (\autoref{DesignVerPlan}), 
the VnV verification plan (\autoref{VnvVerPlan}), 
implementation verification plan (\autoref{ImplementationVerPlan}), 
automated testing and verification tools (\autoref{AutoTestVerTools}), and
Software validation plan (\autoref{SoftwareValPlan}).

% \wss{Introduce this section.   You can provide a roadmap of the sections to
  % come.}

\subsection{Verification and Validation Team}\label{VnVTeam}

The VnV team members and their roles are shown in Table~\ref{VnVTeamTable}.

\begin{center}
  \begin{table}[h]
  \resizebox{\textwidth}{!}{ %
      \begin{tabular}{ |l|l|p{2cm}|p{5cm}| } 
      \hline
      \rowcolor[gray]{0.9}
      \textbf{Name} & \textbf{Document} & \textbf{Role} & \textbf{Description} \\
      \hline
       Dr.\ Spencer Smith & All & Instructor/ Reviewer & Review the documents, design and documentation style. \\ 
       \hline
       Tanya Djavaherpour & All & Author & Create and manage all the documents, create the VnV plan, perform the VnV testing, verify the implementation. \\  
       \hline
       Fatemeh Norouziani & All & Domain Expert Reviewer & Review all the documents. \\  
       \hline
       Atiyeh Sayadi & SRS & Secondary Reviewer & Review the SRS document. \\
       \hline
       Yi-Leng Chen & VnV Plan & Secondary Reviewer & Review the VnV plan. \\ 
       \hline 
       Cynthia Liu & MG + MIS & Secondary Reviewer & Review the MG and MIS document. \\
      \hline 
      
      \hline
      \end{tabular} %
  }
  \caption{Verification and validation team} 
  \label{VnVTeamTable}
  \end{table}
  \end{center}

% \wss{Your teammates.  Maybe your supervisor.
%   You should do more than list names.  You should say what each person's role is
%   for the project's verification.  A table is a good way to summarize this information.}

\subsection{SRS Verification Plan}\label{SRSVerPlan}

The verification process for the Artificial Neural Network's Software Requirements Specification 
document will be conducted as follows:

\begin{enumerate}
  \item An initial review will be carried out by designated team members, which include Dr. 
    Spencer Smith, Fatemeh Norouziani, Atiyeh Sayadi, and Tanya Djavaherpour. 
    This review will utilize a manual method, guided by an 
    SRS Checklist \cite{SRS-Checklist} developed by Dr. Smith.
  \item Reviewers can give feedback and revision suggestions to the author by creating issues on GitHub.
  \item It is the responsibility of the author, Tanya Djavaherpour, to respond to and 
  resolve these issues, incorporating feedback from both primary and secondary reviewers, 
  as well as addressing any recommendations provided by the instructor, Dr. Spencer Smith.
\end{enumerate}

% \wss{List any approaches you intend to use for SRS verification.  This may include
%   ad hoc feedback from reviewers, like your classmates, or you may plan for 
%   something more rigorous/systematic.}

% \wss{Maybe create an SRS checklist?}

\subsection{Design Verification Plan}\label{DesignVerPlan}

The verification of the design documentation, including the Module Guide (MG) 
and Module Interface Specification (MIS), will be conducted via a static 
analysis approach, namely document inspection. As shown in Table~\ref{VnVTeamTable}
this process will be led by 
the domain/primary expert, Fatemeh Norouziani, and supported by the secondary 
reviewer, Cynthia Liu. Additionally, Dr. Spencer Smith, the class instructor, 
will also conduct a review of these documents. Reviewers are encouraged to 
provide their feedback directly to the author by creating issues in the 
project's GitHub repository. It is the responsibility of the author to 
address and resolve these issues, taking into account all the suggestions 
made. The review process will be facilitated by utilizing the 
MG \cite{MG-Checklist} and MIS \cite{MIS-checklist} 
Checklists, which have been formulated by Dr. Spencer Smith.

% \wss{Plans for design verification}

% \wss{The review will include reviews by your classmates}

% \wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}\label{VnvVerPlan}

Following the structure outlined in Table~\ref{VnVTeamTable}, the development and preliminary verification of 
the Verification and Validation plan will be undertaken by Author, Tanya Djavaherpour. 
Subsequent to this phase, domain expert, Fatemeh Norouziani, along with 
Yi-Leng Chen as a secondary reviewer, will review it. 
They give feedback and suggestions via GitHub issues. 
Once done, Instructor will do final review of the VnV plan.
The whole review process will be aligned with the 
VnV Checklist \cite{VnV-Checklist} that Dr. Smith has prepared.
It is the authorâ€™s responsibility to check the submitted issues regularly 
and make necessary modifications.

% \wss{The verification and validation plan is an artifact that should also be
% verified.  Techniques for this include review and mutation testing.}

% \wss{The review will include reviews by your classmates}

% \wss{Create a checklists?}

\subsection{Implementation Verification Plan}\label{ImplementationVerPlan}

The implementation of the software will be verified using several techniques
involving manual and automated techniques as outlined below:

\begin{inparaitem}
  \item \textbf{Static Verification Techniques:}
  Code Walkthroughs will be the primary technique used for static verification. 
  These sessions involve the development team, consisting of the author, Tanya Djavaherpour, 
  and the domain expert, Fatemeh Norouziani, reviewing the code together. 
  The final version of the code will be shared with the domain expert prior 
  to the meeting, during which important test cases will be manually examined. 
  This process allows the domain expert to present findings and raise questions, 
  facilitating a comprehensive review.

  \item \textbf{Dynamic Testing:}
  Evaluation of the code will include both unit and system testing, focusing on 
  the functional and nonfunctional requirements outlined in the
  SRS \cite{SRS} document.
  The tools used for these evaluations will be detailed in \ref{AutoTestVerTools}. Furthermore, the test cases 
  used in system and unit testing will be stated in sections \ref{SystemTest} and 5 (will be added),
  % \ref{UnitTest}, 
  respectively.
  This approach ensures a thorough validation of the software across various levels of testing.

\end{inparaitem}

% \wss{You should at least point to the tests listed in this document and the unit
%   testing plan.}

% \wss{In this section you would also give any details of any plans for static
%   verification of the implementation.  Potential techniques include code
%   walkthroughs, code inspection, static analyzers, etc.}

\subsection{Automated Testing and Verification Tools}\label{AutoTestVerTools}

In this image classification project, Pylint \cite{pylint} and a regular testing process with a 
dedicated testing dataset of CIFAR-10 \cite{CIFAR10} are employed for automated testing and verification. 
Pylint \cite{pylint} is key for enhancing code quality by identifying coding errors, 
enforcing standards, and encouraging best practices. Alongside, a robust 
testing process, involving thorough evaluation on a separate testing dataset, 
ensures the accuracy and reliability of the image classification model. The combination of 
Pylint \cite{pylint} for code quality analysis and performance validation through targeted testing 
provides a comprehensive approach to maintaining robustness and effectiveness in the 
image classification system.

% \wss{What tools are you using for automated testing.  Likely a unit testing
%   framework and maybe a profiling tool, like ValGrind.  Other possible tools
%   include a static analyzer, make, continuous integration tools, test coverage
%   tools, etc.  Explain your plans for summarizing code coverage metrics.
%   Linters are another important class of tools.  For the programming language
%   you select, you should look at the available linters.  There may also be tools
%   that verify that coding standards have been respected, like flake9 for
%   Python.}

% \wss{If you have already done this in the development plan, you can point to
% that document.}

% \wss{The details of this section will likely evolve as you get closer to the
%   implementation.}

\subsection{Software Validation Plan}\label{SoftwareValPlan}

Validation is the process of comparing the outputs of models to experimental values. 
Since the CIFAR-10 dataset \cite{CIFAR10} does not come with a predefined validation 
set and there is not enough experimental data, the validation of the implemented ANN 
cannot be measured in a traditional sense. Instead, as mentioned previously in \ref{AutoTestVerTools}, the network 
will be tested to find the accuracy using testing data. 

It's essential to understand that this approach, while deviating from 
conventional validation methods due 
to dataset constraints, still provides a valid assessment of the model's performance. 
This testing process offers a reliable evaluation of the model's ability to 
generalize and perform accurately on unseen data.

% \wss{If there is any external data that can be used for validation, you should
%   point to it here.  If there are no plans for validation, you should state that
%   here.}

% \wss{You might want to use review sessions with the stakeholder to check that
% the requirements document captures the right requirements.  Maybe task based
% inspection?}

% \wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
% be used as an opportunity to validate the requirements.  You should plan on 
% demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
% The feedback from your supervisor will be very useful for improving your project.}

% \wss{For teams without an external supervisor, user testing can serve the same purpose 
% as a Rev 0 demo for the supervisor.}

% \wss{This section might reference back to the SRS verification section.}

\section{System Test Description}\label{SystemTest}
	
\subsection{Tests for Functional Requirements}

The functional requirements are described in the 
SRS \cite{SRS} section 5.1. 
Implemented ANN will detect invalid inputs and, although the accuracy will not be 100\%,
classes of uploaded image will be determined. There are five functional requirements for
this system. Testing R1 and R2 will be explained in 
the input verification section. R3, R4 and R5 will be
explained in the output verification test.

% \wss{Subsets of the tests may be in related, so this section is divided into
%   different areas.  If there are no identifiable subsets for the tests, this
%   level of document structure can be removed.}

% \wss{Include a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good here.}

\subsubsection{Input Verification} \label{InputVerification}

The inputs will be tested to satisfy R1 and R2 from ANN 
SRS \cite{SRS}. Specifically,
this test will ensure values of the inputs align with the input constraints.
Table~\ref{TCInput} displays the inputs and outputs of test cases for the input constraints
tests.
% \wss{It would be nice to have a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good here.  If a section
%   covers tests for input constraints, you should reference the data constraints
%   table in the SRS.}
		
\paragraph{Input Verification Test} 
\begin{center}
  \begin{table}[h]
  \resizebox{\textwidth}{!}
  { %
      \begin{tabular}{ lccccc }
      \hline
      \multicolumn{1}{l|}{}   & \multicolumn{2}{c|}{Input (an imgae) }                            & \multicolumn{2}{c}{Output} \\ 
      
      \hline
      
      \multicolumn{1}{c|}{ID} &   $type$   &   \multicolumn{1}{c|}{$size$}   &   valid?   &   Error Message \\ \hline
      
      TC-ANN-1   &   PNG  & mxn        &  Y  & NONE               \\
      TC-ANN-2   &   JPG  & mxn        &  Y  & NONE               \\
      TC-ANN-3   &   GIF  & mxn        &  N  & Invalid Input Type  \\ 
      TC-ANN-4   &   PNG  & (m+1)x(n)  &  N  & Invalid Input Size   \\
      TC-ANN-5   &   JPG  & (m+1)x(n)  &  N  & Invalid Input Size    \\
      TC-ANN-6   &   GIF  & (m+1)x(n)  &  N  & Invalid Input Type and Size \\
      TC-ANN-7   &        &            &  N  & Empty      \\
      \hline
      
      
      \end{tabular} %
  }
  \caption{TC-ANN- Test Cases for Input Verification}
  \label{TCInput}
  \end{table}
  \end{center}

\begin{enumerate}

  \item{T1: Valid Inputs\\} 

  Control: Automated Test
  
  Initial State: Pending Input
  
  Input: Set of input values for area of particular object given in the Table~\ref{TCInput}.
  
  Output: Either give an appropriate error message for TC-ANN-1-3 to TC-ANN-1-7,
  or the class of the image object identified by the ANN as an output defined in the Table~\ref{TCInput}.
  
  Test Case Derivation: Justified by the ANN's training to accurately classify 
  valid input images according to the 
  SRS \cite{SRS} specifications.

  How test will be performed: An automated script will input valid and invalid images to the ANN, 
  comparing the output classifications with expected results to verify accuracy.

\end{enumerate}

\subsubsection{Output Verification Test} \label{OutputVerificationTest}
To satisfy R3, R4 and R5 from the 
SRS \cite{SRS}, any input image 
should be properly classified and related to the correct output label
from the set of 10 classes of CIFAR-10 \cite{CIFAR10}.

\begin{enumerate}

  \item{T2: Classifier Test\\} \label{T2}

  Control: Automated Test
  
  Initial State: Loading Trained ANN
  
  Input: One image with one object
  
  Output: Class of the image object
  
  Test Case Derivation: The test is designed to evaluate the ANN's precision in classifying an 
  individual image, reflecting its real-world application for singular image analysis.

  How test will be performed: This test will be conducted using an automated script that 
  inputs a single, randomly selected test image with one object into the trained ANN. 
  The output will be the classified label provided by 
   the ANN, which will then be compared to the actual label of the image to verify the accuracy of the classification.

\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

NonFunctional requirements for ANN are given in 
SRS \cite{SRS} section 5.2.

% \wss{The nonfunctional requirements for accuracy will likely just reference the
%   appropriate functional tests from above.  The test cases should mention
%   reporting the relative error for these tests.  Not all projects will
%   necessarily have nonfunctional requirements related to accuracy}

% \wss{Tests related to usability could include conducting a usability test and
%   survey.  The survey will be in the Appendix.}

% \wss{Static tests, review, inspections, and walkthroughs, will not follow the
% format for the tests given below.}

\subsubsection{Nonfunctional: Accuracy} \label{NFAccuracy}
		
\paragraph{Accuracy}

\begin{enumerate}

  \item{T3: Accuracy Test\\}

  Control: Automated Test
  
  Initial State: Loading Trained ANN
  
  Input: Set of test images form the CIFAR-10 dataset \cite{CIFAR10}
  
  Output: System accuracy based on prediction of class of the image object

  How test will be performed: The test will be executed using an automated 
  script that feeds a batch of test images from the CIFAR-10 dataset \cite{CIFAR10} 
  into the trained ANN. 
  The script will then compare the ANN's predicted class for each image against 
  the known label, calculating the overall accuracy of the system. This process 
  will quantify the model's performance in terms of correct classifications, 
  providing a clear measure of its effectiveness in fulfilling the specified requirements.

\end{enumerate}

\subsubsection{Nonfunctional: Usability} \label{NFUsability}
		
\paragraph{Usability}

\begin{enumerate}

  \item{T4: Usability Test\\}

  Control: Manual with group of people
  
  Initial State: None
  
  Input: None
  
  Output: A survey to gather user perspectives on the system's usability

  How test will be performed: A diverse group of users will be asked to install 
  and interact with the software, performing specified tasks. 
  Following this, they will complete a survey focusing on aspects of 
  usability such as ease of use, intuitiveness, and overall satisfaction. 
  The survey results will be analyzed to identify areas for improvement in the software's usability. 
  The questions are given in Table~\ref{UsabilitySurvey}.
\end{enumerate}

\begin{table}[h!]
  \begin{center}
  \begin{tabular}{ p{0.5cm}|p{10cm}|c }
  \hline
  No. &  Question   & Answer \\
  \hline
  1. & Which operating system are you using?  & \\
  2. & Was system running smoothly on your computer?  & \\
  3. & How would you rate the response time of the software for various operations? (On a scale of 1-10) & \\
  4. & How does the software handle errors or unexpected user actions? & \\
  5. & During using the software, were received errors clear to recover them, if you received any? & \\
  6. & Were you able to find all the functionalities easily? & \\
  7. & Was there sufficient help or documentation available? & \\
  8. & What, if anything, surprised you about the experience? & \\
  9. & What specific feature or aspect of the software did you find least effective or most challenging? & \\
  10. & What improvements or additional features would you suggest for enhancing the software's functionality or user experience?  & \\
  11. & How likely are you to recommend this software to others? (On a scale of 1-10) & \\
  \hline
  \end{tabular}
  \caption{Usability Test Survey}
  \label{UsabilitySurvey}
  \end{center}
  \end{table}

\subsubsection{Nonfunctional: Maintainability} \label{NFMaintainablity}
		
\paragraph{Maintainability}

\begin{enumerate}

  \item{T5: Code Walkthrough Maintainability Test\\}

  Control: Code Walkthrough
  
  Initial State: None
  
  Input: None
  
  Output: Walkthrough Meeting aimed at identifying areas for improvement in 
  system maintainability and documentation quality.

  How test will be performed: In the code walkthrough meeting, team members 
  (referenced in Table~\ref{VnVTeamTable}) will review the software's code and documentation, 
  focusing on readability, modularity, and documentation clarity. Key findings and action 
  points for enhancing maintainability will be documented for future reference and implementation.

  \item{T6: Automatic Maintainability Test\\}

  Control: Automatic
  
  Initial State: None
  
  Input: None
  
  Output: Improve code quality with Pylint \cite{pylint}.

  How test will be performed: Pylint \cite{pylint} will be used to check the code quality.
  This library can check the quality of code against different coding styles such as unused libraries
  and undefined variable. Which leads to easier maintenance.
\end{enumerate}

\subsubsection{Nonfunctional: Portability} \label{NFPortablity}
		
\paragraph{Portability}

\begin{enumerate}

  \item{T7: Portability Test\\}

  Control: Manual
  
  Initial State: None
  
  Input: None
  
  Output: Successful running system over all platforms with different operating environments

  How test will be performed: The Author (Tanya Djavaherpour) will try to install and run
  whole software on different operating systems. Also, need to ensure regression testing
  that means all the given test cases pass on all different operating system.
\end{enumerate}


\subsection{Traceability Between Test Cases and Requirements}

The traceability between test cases and requirements is indicated in \autoref{Traceability} 



\begin{table}[h!]
\begin{center}
\begin{tabular}{ l|c|c|c|c|c|c|c|c|c }
\hline
 & R1 & R2 & R3 & R4 & R5 & NFR1 & NFR2 & NFR3 & NFR4 \\
\hline
\ref{InputVerification} & X & X & & & & & & \\
\hline
\ref{OutputVerificationTest} & & & X & X & X & & & \\
\hline
\ref{NFAccuracy} & & & & & & X & & \\
\hline
\ref{NFUsability} & & & & & & & X & \\
\hline
\ref{NFMaintainablity} & & & & & & & & X & \\
\hline
\ref{NFPortablity} & & & & & & & & & X \\
\hline
\end{tabular}
\caption{Tracebility between test cases and requirements}
\label{Traceability}
\end{center}
\end{table}
% \wss{Provide a table that shows which test cases are supporting which
%   requirements.}

% \section{Unit Test Description}\label{UnitTest}

% \wss{This section should not be filled in until after the MIS (detailed design
%   document) has been completed.}

% \wss{Reference your MIS (detailed design document) and explain your overall
% philosophy for test case selection.}  

% \wss{To save space and time, it may be an option to provide less detail in this section.  
% For the unit tests you can potentially layout your testing strategy here.  That is, you 
% can explain how tests will be selected for each module.  For instance, your test building 
% approach could be test cases for each access program, including one test for normal behaviour 
% and as many tests as needed for edge cases.  Rather than create the details of the input 
% and output here, you could point to the unit testing code.  For this to work, you code 
% needs to be well-documented, with meaningful names for all of the tests.}

% \subsection{Unit Testing Scope}

% \wss{What modules are outside of the scope.  If there are modules that are
%   developed by someone else, then you would say here if you aren't planning on
%   verifying them.  There may also be modules that are part of your software, but
%   have a lower priority for verification than others.  If this is the case,
%   explain your rationale for the ranking of module importance.}

% \subsection{Tests for Functional Requirements}

% \wss{Most of the verification will be through automated unit testing.  If
%   appropriate specific modules can be verified by a non-testing based
%   technique.  That can also be documented in this section.}

% \subsubsection{Module 1}

% \wss{Include a blurb here to explain why the subsections below cover the module.
%   References to the MIS would be good.  You will want tests from a black box
%   perspective and from a white box perspective.  Explain to the reader how the
%   tests were selected.}

% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input: 
					
% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed: 
					
% \item{test-id2\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input: 
					
% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed: 

% \item{...\\}
    
% \end{enumerate}

% \subsubsection{Module 2}

% ...

% \subsection{Tests for Nonfunctional Requirements}

% \wss{If there is a module that needs to be independently assessed for
%   performance, those test cases can go here.  In some projects, planning for
%   nonfunctional tests of units will not be that relevant.}

% \wss{These tests may involve collecting performance data from previously
%   mentioned functional tests.}

% \subsubsection{Module ?}
		
% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input/Condition: 
					
% Output/Result: 
					
% How test will be performed: 
					
% \item{test-id2\\}

% Type: Functional, Dynamic, Manual, Static etc.
					
% Initial State: 
					
% Input: 
					
% Output: 
					
% How test will be performed: 

% \end{enumerate}

% \subsubsection{Module ?}

% ...

% \subsection{Traceability Between Test Cases and Modules}

% \wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

% \section{Appendix}

% This is where you can place additional information.

% \subsection{Symbolic Parameters}

% The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
% Their values are defined in this section for easy maintenance.

% \subsection{Usability Survey Questions?}

% \wss{This is a section that would be appropriate for some projects.}

% \newpage{}
% \section*{Appendix --- Reflection}

% The information in this section will be used to evaluate the team members on the
% graduate attribute of Lifelong Learning.  Please answer the following questions:

% \newpage{}
% \section*{Appendix --- Reflection}

% \wss{This section is not required for CAS 741}

% The information in this section will be used to evaluate the team members on the
% graduate attribute of Lifelong Learning.  Please answer the following questions:

% \begin{enumerate}
%   \item What knowledge and skills will the team collectively need to acquire to
%   successfully complete the verification and validation of your project?
%   Examples of possible knowledge and skills include dynamic testing knowledge,
%   static testing knowledge, specific tool usage etc.  You should look to
%   identify at least one item for each team member.
%   \item For each of the knowledge areas and skills identified in the previous
%   question, what are at least two approaches to acquiring the knowledge or
%   mastering the skill?  Of the identified approaches, which will each team
%   member pursue, and why did they make this choice?
% \end{enumerate}

\end{document}