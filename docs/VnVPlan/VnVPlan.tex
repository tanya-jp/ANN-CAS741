\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{paralist}
\usepackage{amsmath, mathtools}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for ANN} 
\author{Tanya Djavaherpour}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Feb. 15, 2024 & 1.0 & Initial Draft\\
Mar. 04, 2024 & 1.1 & Modification According to the Feedback 1\\
Apr. 09, 2024 & 1.2 & Modification According to the Feedback 2\\
Apr. 10, 2024 & 1.3 & Modification According to Dr. Smith's Feedback\\
Apr. 15, 2024 & 1.4 & Complete Unit Test\\
\bottomrule
\end{tabularx}

~\\
% \wss{The intention of the VnV plan is to increase confidence in the software.
% However, this does not mean listing every verification and validation technique
% that has ever been devised.  The VnV plan should also be a \textbf{feasible}
% plan. Execution of the plan should be possible with the time and team available.
% If the full plan cannot be completed during the time available, it can either be
% modified to ``fake it'', or a better solution is to add a section describing
% what work has been completed and what work is still planned for the future.}

% \wss{The VnV plan is typically started after the requirements stage, but before
% the design stage.  This means that the sections related to unit testing cannot
% initially be completed.  The sections will be filled in after the design stage
% is complete.  the final version of the VnV plan should have all sections filled
% in.}

\newpage

\tableofcontents

\listoftables
% \wss{Remove this section if it isn't needed}

% \listoffigures
% \wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  ANN & Artificial Neural Network\\
  IM & Instance Model\\
  MG & Module Guide\\
  MIS & Module Interface Specification\\
  N & No \\
  SRS & Software Requirements Specification\\
  T & Test\\
  VnV & Verification and Validation\\
  Y & Yes\\

  \bottomrule
\end{tabular}\\

For complete symbols used within the system, please refer the section 1 in 
SRS \citep{SRS} document.

% \wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
%   \citep{SRS} tables, if appropriate}

% \wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document outlines the Verification and Validation (VnV) plan for the Artificial 
Neural Network for Image Classification project, as detailed in the 
SRS \citep{SRS}. 
The purpose of this VnV plan is to ensure that all requirements and objectives outlined 
in the SRS \citep{SRS} 
are met with accuracy and efficiency.

The organization of this document starts with the General Information about the ANN in \autoref{GeneralInformation}. 
A verification plan is provided in \autoref{Plan} and \autoref{SystemTest} 
describes the system tests, including tests for functional and nonfunctional requirements. 
Test Description is explained in section 5 (will be added).
% \autoref{UnitTest}.

\section{General Information}\label{GeneralInformation}

This Verification and Validation (VnV) Plan outlines the procedures and 
criteria to be used for ensuring the quality and reliability of the Artificial 
Neural Network (ANN) developed for Image Classification. Central to this plan is the 
systematic verification of the software's adherence to its requirements and specifications, 
as detailed in the Software Requirements Specification \citep{SRS}, and the validation of its 
effectiveness in accurately classifying images within the scope of the CIFAR-10 dataset \citep{CIFAR10}. 
This VnV process is crucial to affirm that the ANN system meets both its technical and user-centric goals.

\subsection{Summary}

The software being validated in this plan is an Artificial Neural Network designed for 
Image Classification, tailored specifically to work with the CIFAR-10 dataset \citep{CIFAR10}. 
It allows users to upload images and efficiently classifies them into predefined 
categories. It operates within the constraints of available computational 
resources and is limited to handling images that fall under the CIFAR-10 dataset \citep{CIFAR10}
categories, ensuring focused and optimized performance in its designated area.

\subsection{Objectives}

The primary objective of this VnV plan is to build confidence in the correctness and 
reliability of the Artificial Neural Network for Image Classification. Our goal is to 
demonstrate that the system can classify images with a high degree of accuracy. We aim 
to significantly improve upon the less than 50\% accuracy achieved in 
previous implementation \citep{ANN:CIFAR10}, 
acknowledging that reaching 100\% accuracy is not feasible due to inherent limitations in 
ANN models and the variability of image data. The focus will be on achieving the highest 
possible accuracy within these constraints. The system's accuracy will be measured through 
defined quantitative methods such as the cost function in SRS \citep{SRS}.

% \wss{State what is intended to be accomplished.  The objective will be around
%   the qualities that are most important for your project.  You might have
%   something like: ``build confidence in the software correctness,''
%   ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
%   just those that are most important.}

% \wss{You should also list the objectives that are out of scope.  You don't have 
% the resources to do everything, so what will you be leaving out.  For instance, 
% if you are not going to verify the quality of usability, state this.  It is also 
% worthwhile to justify why the objectives are left out.}

% \wss{The objectives are important because they highlight that you are aware of 
% limitations in your resources for verification and validation.  You can't do everything, 
% so what are you going to prioritize?  As an example, if your system depends on an 
% external library, you can explicitly state that you will assume that external library 
% has already been verified by its implementation team.}

\subsection{Relevant Documentation}

The ANN project is supported by several crucial documents. 
These include a
Problem Statement \citep{ProblemStatement}, which introduces the initial concept, and a 
Software Requirements Specification \citep{SRS} that outlines the necessary system requirements, accompanied 
by a Verification and Validation Report \citep{VnVReport} to ensure the system's compliance and efficacy.

% \wss{Reference relevant documentation.  This will definitely include your SRS
%   and your other project documents (design documents, like MG, MIS, etc).  You
%   can include these even before they are written, since by the time the project
%   is done, they will be written.}

% \citet{SRS}

% \wss{Don't just list the other documents.  You should explain why they are relevant and 
% how they relate to your VnV efforts.}

\section{Plan}\label{Plan}

In this section, the VnV plan of ANN is described. 
It begins with an introduction to the verification and validation team (\autoref{VnVTeam}) and 
introduces the members and thei rules. 
Then is followed by the SRS verification plan (\autoref{SRSVerPlan}), 
design verification plan (\autoref{DesignVerPlan}), 
the VnV verification plan (\autoref{VnvVerPlan}), 
implementation verification plan (\autoref{ImplementationVerPlan}), 
automated testing and verification tools (\autoref{AutoTestVerTools}), and
Software validation plan (\autoref{SoftwareValPlan}).

% \wss{Introduce this section.   You can provide a roadmap of the sections to
  % come.}

\subsection{Verification and Validation Team}\label{VnVTeam}

The VnV team members and their roles are shown in Table~\ref{VnVTeamTable}.

\begin{center}
  \begin{table}[h]
  \resizebox{\textwidth}{!}{ %
      \begin{tabular}{ |l|l|p{2cm}|p{5cm}| } 
      \hline
      \rowcolor[gray]{0.9}
      \textbf{Name} & \textbf{Document} & \textbf{Role} & \textbf{Description} \\
      \hline
       Dr.\ Spencer Smith & All & Instructor/ Reviewer & Review the documents, design and documentation style. \\ 
       \hline
       Tanya Djavaherpour & All & Author & Create and manage all the documents, create the VnV plan, perform the VnV testing, verify the implementation. \\  
       \hline
       Fatemeh Norouziani & All & Domain Expert Reviewer & Review all the documents. \\  
       \hline
       Atiyeh Sayadi & SRS & Secondary Reviewer & Review the SRS document. \\
       \hline
       Yi-Leng Chen & VnV Plan & Secondary Reviewer & Review the VnV plan. \\ 
       \hline 
       Cynthia Liu & MG + MIS & Secondary Reviewer & Review the MG and MIS document. \\
      \hline 
      
      \hline
      \end{tabular} %
  }
  \caption{Verification and validation team} 
  \label{VnVTeamTable}
  \end{table}
  \end{center}

% \wss{Your teammates.  Maybe your supervisor.
%   You should do more than list names.  You should say what each person's role is
%   for the project's verification.  A table is a good way to summarize this information.}

\subsection{SRS Verification Plan}\label{SRSVerPlan}

The verification process for the Artificial Neural Network's Software Requirements Specification 
document will be conducted as follows:

\begin{enumerate}
  \item An initial review will be carried out by designated team members, which include Dr. 
    Spencer Smith, Fatemeh Norouziani, Atiyeh Sayadi, and Tanya Djavaherpour. 
    This review will utilize a manual method, guided by an 
    SRS Checklist \citep{SRS-Checklist} developed by Dr. Smith.
  \item Reviewers can give feedback and revision suggestions to the author by creating issues on GitHub.
  \item It is the responsibility of the author, Tanya Djavaherpour, to respond to and 
  resolve these issues, incorporating feedback from both primary and secondary reviewers, 
  as well as addressing any recommendations provided by the instructor, Dr. Spencer Smith.
\end{enumerate}

% \wss{List any approaches you intend to use for SRS verification.  This may include
%   ad hoc feedback from reviewers, like your classmates, or you may plan for 
%   something more rigorous/systematic.}

% \wss{Maybe create an SRS checklist?}

\subsection{Design Verification Plan}\label{DesignVerPlan}

The verification of the design documentation, including the Module Guide (MG) 
and Module Interface Specification (MIS), will be conducted via a static 
analysis approach, namely document inspection. As shown in Table~\ref{VnVTeamTable}
this process will be led by 
the domain/primary expert, Fatemeh Norouziani, and supported by the secondary 
reviewer, Cynthia Liu. Additionally, Dr. Spencer Smith, the class instructor, 
will also conduct a review of these documents. Reviewers are encouraged to 
provide their feedback directly to the author by creating issues in the 
project's GitHub repository. It is the responsibility of the author to 
address and resolve these issues, taking into account all the suggestions 
made. The review process will be facilitated by utilizing the 
MG \citep{MG-Checklist} and MIS \citep{MIS-checklist} 
Checklists, which have been formulated by Dr. Spencer Smith.

% \wss{Plans for design verification}

% \wss{The review will include reviews by your classmates}

% \wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}\label{VnvVerPlan}

Following the structure outlined in Table~\ref{VnVTeamTable}, the development and preliminary verification of 
the Verification and Validation plan will be undertaken by Author, Tanya Djavaherpour. 
Subsequent to this phase, domain expert, Fatemeh Norouziani, along with 
Yi-Leng Chen as a secondary reviewer, will review it. 
They give feedback and suggestions via GitHub issues. 
Once done, Instructor will do final review of the VnV plan.
The whole review process will be aligned with the 
VnV Checklist \citep{VnV-Checklist} that Dr. Smith has prepared. 
One specefic template has been defined for this purpose on the 
\href{https://github.com/tanya-jp/ANN-CAS741}{GitHub repository}.
It is the author's responsibility to check the submitted issues regularly 
and make necessary modifications.

% \wss{The verification and validation plan is an artifact that should also be
% verified.  Techniques for this include review and mutation testing.}

% \wss{The review will include reviews by your classmates}

% \wss{Create a checklists?}

\subsection{Implementation Verification Plan}\label{ImplementationVerPlan}

The implementation of the software will be verified using several techniques
involving manual and automated techniques as outlined below:

\begin{inparaitem}
  \item \textbf{Static Verification Techniques:}
  Code Walkthroughs will be the primary technique used for static verification. 
  These sessions involve the development team, consisting of the author, Tanya Djavaherpour, 
  and the domain expert, Fatemeh Norouziani, reviewing the code together. 
  Initially, th author, Djavaherpour, thoroughly reviews the code independently, pinpointing 
  possible issues. The final version of the code will be shared with the domain expert prior 
  to the meeting, during which important test cases will be manually examined. 
  In our static verification approach, we use code walkthroughs, 
  guided by domain expert Fatemeh Norouziani. 
  In the walkthrough meetings, we collaboratively discuss these 
  points, focusing on logical inconsistencies and adherence to standards. 
  The discussions and resolutions are then documented systematically. 
  Following this, the author addresses the highlighted issues, ensuring all modifications are well-documented. 
  If substantial issues are detected, we schedule an additional walkthrough to verify the resolutions. 
  This methodical process is crucial for ensuring the software's adherence to the project's quality standards.
  Also, this process allows the domain expert to present findings and raise questions, 
  facilitating a comprehensive review.

  \item \textbf{Dynamic Testing:}
  Evaluation of the code will include both unit and system testing, focusing on 
  the functional and nonfunctional requirements outlined in the
  SRS \citep{SRS} document.
  The tools used for these evaluations will be detailed in \ref{AutoTestVerTools}. Furthermore, the test cases 
  used in system and unit testing will be stated in sections \ref{SystemTest} and 5 (will be added),
  % \ref{UnitTest}, 
  respectively.
  This approach ensures a thorough validation of the software across various levels of testing.

\end{inparaitem}

% \wss{You should at least point to the tests listed in this document and the unit
%   testing plan.}

% \wss{In this section you would also give any details of any plans for static
%   verification of the implementation.  Potential techniques include code
%   walkthroughs, code inspection, static analyzers, etc.}

\subsection{Automated Testing and Verification Tools}\label{AutoTestVerTools}

In this image classification project, Pylint \citep{pylint} and a regular testing process with a 
dedicated testing dataset of CIFAR-10 \citep{CIFAR10} are employed for automated testing and verification. 
Pylint \citep{pylint} is thorough in identifying coding issues, including syntax errors, 
logical bugs, and signs of problematic coding patterns, like complexity or redundancy. 
It upholds the PEP 8 style guide, promoting best practices in code formatting and naming. 
Pylint \citep{pylint} also supports type checking for annotated projects, identifies unused 
code, and flags potential string format errors. Security-wise, it alerts on usage of risky 
functions such as "exec" or "eval" and checks for documentation quality. Its flexibility 
in configuration allows customization according to project needs, playing a key role in 
sustaining the quality of our Python code. The combination of 
Pylint \citep{pylint} for code quality analysis and performance validation through targeted testing 
provides a comprehensive approach to maintaining robustness and effectiveness in the 
image classification system.

% \wss{What tools are you using for automated testing.  Likely a unit testing
%   framework and maybe a profiling tool, like ValGrind.  Other possible tools
%   include a static analyzer, make, continuous integration tools, test coverage
%   tools, etc.  Explain your plans for summarizing code coverage metrics.
%   Linters are another important class of tools.  For the programming language
%   you select, you should look at the available linters.  There may also be tools
%   that verify that coding standards have been respected, like flake9 for
%   Python.}

% \wss{If you have already done this in the development plan, you can point to
% that document.}

% \wss{The details of this section will likely evolve as you get closer to the
%   implementation.}

\subsection{Software Validation Plan}\label{SoftwareValPlan}

Considering the extensive amount of experimental data required for a thorough software 
validation, this process falls outside the practical scope of our ANN project. 
The constraints of time make it infeasible to collect and analyze the necessary 
data to validate system behavior comprehensively. However, we will ensure that the 
system undergoes rigorous verification to maintain high standards of quality and 
performance within these constraints.

% Validation is the process of comparing the outputs of models to experimental values. 
% Since the CIFAR-10 dataset \citep{CIFAR10} does not come with a predefined validation 
% set and there is not enough experimental data, the validation of the implemented ANN 
% cannot be measured in a traditional sense. Instead, as mentioned previously in \ref{AutoTestVerTools}, the network 
% will be tested to find the accuracy using testing data. 

% It's essential to understand that this approach, while deviating from 
% conventional validation methods due 
% to dataset constraints, still provides a valid assessment of the model's performance. 
% This testing process offers a reliable evaluation of the model's ability to 
% generalize and perform accurately on unseen data.

% \wss{If there is any external data that can be used for validation, you should
%   point to it here.  If there are no plans for validation, you should state that
%   here.}

% \wss{You might want to use review sessions with the stakeholder to check that
% the requirements document captures the right requirements.  Maybe task based
% inspection?}

% \wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
% be used as an opportunity to validate the requirements.  You should plan on 
% demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
% The feedback from your supervisor will be very useful for improving your project.}

% \wss{For teams without an external supervisor, user testing can serve the same purpose 
% as a Rev 0 demo for the supervisor.}

% \wss{This section might reference back to the SRS verification section.}

\section{System Test Description}\label{SystemTest}

In this section, we outline the test strategies for validating the ANN system's functional 
and nonfunctional requirements. The tests are designed to ensure compliance with the 
specifications detailed in the SRS, encompassing all aspects of system performance and quality.
	
\subsection{Tests for Functional Requirements} \label{T1T2}

The functional requirements are described in the 
SRS \citep{SRS} section 5.1. 
Implemented ANN will detect invalid inputs and, although the accuracy will not be 100\%,
classes of uploaded image will be determined. There are five functional requirements for
this system. Testing R1 and R2 will be explained in 
the input verification section. R3 will be
explained in the output verification test.

% \wss{Subsets of the tests may be in related, so this section is divided into
%   different areas.  If there are no identifiable subsets for the tests, this
%   level of document structure can be removed.}

% \wss{Include a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good here.}

\subsubsection{Input Verification} \label{InputVerification}

The inputs will be tested to satisfy R1 and R2 from ANN 
SRS \citep{SRS}. Specifically,
this test will ensure values of the inputs align with the input constraints.
Table~\ref{TCInput} displays the inputs and outputs of test cases for the input constraints
tests.
% \wss{It would be nice to have a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good here.  If a section
%   covers tests for input constraints, you should reference the data constraints
%   table in the SRS.}
		
\paragraph{Input Verification Test} 
\begin{center}
  \begin{table}[h]
  \resizebox{\textwidth}{!}
  { %
      \begin{tabular}{ lccccc }
      \hline
      \multicolumn{1}{l|}{}   & \multicolumn{2}{c|}{Input (an imgae) }                            & \multicolumn{2}{c}{Output} \\ 
      
      \hline
      
      \multicolumn{1}{c|}{ID} &   $type$   &   \multicolumn{1}{c|}{$size$}   &   valid?   &   Error Message \\ \hline
      
      TC-ANN-1   &   PNG  & mxn        &  Y  & NONE               \\
      TC-ANN-2   &   JPG  & mxn        &  Y  & NONE               \\
      TC-ANN-3   &   GIF  & mxn        &  N  & Invalid Input Type  \\ 
      TC-ANN-4   &   PNG  & (m+1)x(n)  &  N  & Invalid Input Size   \\
      TC-ANN-5   &   JPG  & (m+1)x(n)  &  N  & Invalid Input Size    \\
      TC-ANN-6   &   GIF  & (m+1)x(n)  &  N  & Invalid Input Type \\
      TC-ANN-7   &        &            &  N  & Empty      \\
      \hline
      
      
      \end{tabular} %
  }
  \caption{TC-ANN- Test Cases for Input Verification}
  \label{TCInput}
  \end{table}
  \end{center}

\begin{enumerate}

  \item{T1: Valid Inputs\\} 

  Control: Automated Test
  
  Initial State: Pending Input
  
  Input: Set of input values for area of particular object given in the Table~\ref{TCInput}.
  
  Output: Either give an appropriate error message for TC-ANN-1-3 to TC-ANN-1-7,
  or the class of the image object identified by the ANN as an output defined in the Table~\ref{TCInput}. 
  It must be mentioned that if the type is not valid, the exception will be raised and the size will not 
  be checked (like TC-ANN-6).

  Test Case Derivation: Justified by the ANN's training to accurately classify 
  valid input images according to the 
  SRS \citep{SRS} specifications.

  How test will be performed: An automated script will input valid and invalid images to the ANN. 
  This automated test works with Pytest \citep{pytest}.

\end{enumerate}

\subsubsection{Output Verification Test} \label{OutputVerificationTest}
To satisfy R3 from the 
SRS \citep{SRS}, any input image 
should be properly classified and related to the correct output label
from the set of 10 classes of CIFAR-10 \citep{CIFAR10}.

\begin{enumerate}

  \item{T2: Classifier Test\\} \label{T2}

  Control: Automated Test
  
  Initial State: Loading Trained ANN
  
  Input: One image with one object
  
  Output: Class of the image object
  
  Test Case Derivation: The test is designed to evaluate the ANN's precision in classifying an 
  individual image, reflecting its real-world application for singular image analysis.

  How test will be performed: This test will be conducted using an automated script that 
  inputs a single, randomly selected test image with one object into the trained ANN. 
  The output will be the classified label provided by 
   the ANN, which will then be compared to the actual label of the image to verify the accuracy of the classification. 
   This test will be run 10 times and in each time the random image will be chosen from one folders. That said, 
   after this pocess the software's perforamnec will be tested for 
   all of the 10 classes of CIFAR-10 dataset \citep{CIFAR10}, including airplane, 
   automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The purpose of this test is to make sure that 
   system can print all of the classes' name properly.

\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

NonFunctional requirements for ANN are given in 
SRS \citep{SRS} section 5.2.

% \wss{The nonfunctional requirements for accuracy will likely just reference the
%   appropriate functional tests from above.  The test cases should mention
%   reporting the relative error for these tests.  Not all projects will
%   necessarily have nonfunctional requirements related to accuracy}

% \wss{Tests related to usability could include conducting a usability test and
%   survey.  The survey will be in the Appendix.}

% \wss{Static tests, review, inspections, and walkthroughs, will not follow the
% format for the tests given below.}

\subsubsection{Nonfunctional: Accuracy} \label{NFAccuracy}
		
\paragraph{Accuracy}

\begin{enumerate}

  \item{T3: Accuracy Test\\}

  Control: Automated Test
  
  Initial State: Loading Trained ANN
  
  Input: Set of test images form the CIFAR-10 dataset \citep{CIFAR10}
  
  Output: System accuracy based on prediction of class of the image object

  How test will be performed: The test will be executed using an automated 
  script that feeds a batch of test images from the CIFAR-10 dataset \citep{CIFAR10} 
  into the trained ANN. 
  The script will then compare the ANN's predicted class for each image against 
  the known label, calculating the overall accuracy of the system. This process 
  will quantify the model's performance in terms of correct classifications, 
  providing a clear measure of its effectiveness in fulfilling the specified requirements. 
  During this test, we just have the accuracy percentage to find how good is the trained network's performance. 
  In fact, accuracy is the percentage of correct predictions.

\end{enumerate}

\subsubsection{Nonfunctional: Usability} \label{NFUsability}
		
\paragraph{Usability}

\begin{enumerate}

  \item{T4: Usability Test\\}

  Control: Manual with group of Computer Science students at McMaster
  
  Initial State: None
  
  Input: None
  
  Output: None

  How test will be performed: A diverse group of users will be asked to install 
  and interact with the software, performing specified tasks. 
  Following this, they will complete a survey focusing on aspects of 
  usability such as ease of use, intuitiveness, and overall satisfaction. 
  The survey results will be analyzed to identify areas for improvement in the software's usability. 
  The questions are given in Table~\ref{UsabilitySurvey}.
\end{enumerate}

\begin{table}[h!]
  \begin{center}
  \begin{tabular}{ p{0.5cm}|p{10cm}|c }
  \hline
  No. &  Question   & Answer \\
  \hline
  1. & Which operating system are you using?  & \\
  2. & Was system running smoothly on your computer? (Y or N) & \\
  3. & How would you rate the response time of the software for various operations? (On a scale of 1-10) & \\
  4. & How does the software handle errors or unexpected user actions? & \\
  5. & During using the software, were received errors clear to recover them, if you received any? & \\
  6. & Were you able to find all the functionalities easily? & \\
  7. & Was there sufficient help or documentation available? & \\
  8. & What, if anything, surprised you about the experience? & \\
  9. & What specific feature or aspect of the software did you find least effective or most challenging? & \\
  10. & What improvements or additional features would you suggest for enhancing the software's functionality or user experience?  & \\
  11. & How likely are you to recommend this software to others? (On a scale of 1-10) & \\
  \hline
  \end{tabular}
  \caption{Usability Test Survey}
  \label{UsabilitySurvey}
  \end{center}
  \end{table}

\subsubsection{Nonfunctional: Maintainability} \label{NFMaintainablity}
		
\paragraph{Maintainability}

\begin{enumerate}

  \item{T5: Code Walkthrough Maintainability Test\\}

  Control: Code Walkthrough
  
  Initial State: None
  
  Input: None
  
  Output: Walkthrough Meeting aimed at identifying areas for improvement in 
  system maintainability and documentation quality.

  How test will be performed: In the code walkthrough meeting, team members 
  (referenced in Table~\ref{VnVTeamTable}) will review the software's code and documentation, 
  focusing on readability, modularity, and documentation clarity. Key findings and action 
  points for enhancing maintainability will be documented for future reference and implementation.

  \item{T6: Automatic Maintainability Test\\}

  Control: Automatic
  
  Initial State: None
  
  Input: None
  
  Output: Improve code quality with Pylint \citep{pylint}.

  How test will be performed: Pylint \citep{pylint} library can check the quality of code 
  against different coding styles such as unused libraries
  and undefined variable. Which leads to easier maintenance.
\end{enumerate}

\subsubsection{Nonfunctional: Portability} \label{NFPortablity}
		
\paragraph{Portability}

\begin{enumerate}

  \item{T7: Portability Test\\}

  Control: Manual
  
  Initial State: None
  
  Input: None
  
  Output: Successful running system over all platforms with different operating environments, 
  including Windows and mcOS.

  How test will be performed: The Author (Tanya Djavaherpour) will try to install and run
  whole software on different operating systems. Also, need to ensure regression testing
  that means all the given test cases pass on all different operating system.
\end{enumerate}


\subsection{Traceability Between Test Cases and Requirements}

The traceability between test cases and requirements is indicated in \autoref{Traceability} 



\begin{table}[h!]
\begin{center}
\begin{tabular}{ |l|c|c|c|c|c|c|c|c|c|c}
\hline
 & R1 & R2 & R3 & NFR1 & NFR2 & NFR3 & NFR4\\
\hline
\ref{InputVerification} & X & X & & & & &\\
\hline
\ref{OutputVerificationTest} & & & X & & & &\\
\hline
\ref{NFAccuracy} & & & & X & &  & \\
\hline
\ref{NFUsability} & & & & & X & & \\
\hline
\ref{NFMaintainablity} & & & & & & X &\\
\hline
\ref{NFPortablity} & & & & & & & X\\
\hline
\end{tabular}
\caption{Tracebility between test cases and requirements}
\label{Traceability}
\end{center}
\end{table}
% \wss{Provide a table that shows which test cases are supporting which
%   requirements.}

\section{Unit Test Description}\label{UnitTest}

This section includes unit testing for ANN modules. 
These modules are available in MG \citep{MG} and 
MIS \citep{MIS} documents.

\subsection{Unit Testing Scope}

The unit tests cover all of the modules. 
These unit tests are directly applied to:

\begin{itemize}
  \item ANN Control Module (M2)
  \item Saved ANN Model Module (M3)
  \item Output Module (M4)
  \item Data Preparing and Preprocessing Module (M9)
\end{itemize}

\subsection{Tests for Functional Requirements}

R1 and R3 are tested with T1 and T2 respectively in \ref{T1T2} using Pytets \citep{pytest}. 
Functional tests not automated by PyTest \citep{pytest} are described in this section.


\subsubsection{ ANN Control Module (M2)}
ANN Control Module is the main executable script for training a model or classifying an image.

\begin{enumerate}
  \item{test-id1\\}
  \textbf{Type:} Automatic \\
  \textbf{Initial State:} N/A \\
  \textbf{Input:} 1 to train the model \\
  \textbf{Output:} Pass if the model is trained correctly, and Fail if it is not. \\
  \textbf{Test Case Derivation:} 
  The expected value in the Output field is justified by the following reasoning:
  \begin{itemize}
      \item The main function should instantiate a Model object and call its save\_model 
      method when the user selects option '1' to train the model.
      \item Therefore, if the main function executes successfully, it indicates that the 
      model is trained or loaded correctly, and the save\_model method is called, resulting in a Pass.
  \end{itemize}
  \textbf{How test will be performed:} 
  Run the main function with input '1' to simulate training the model and verify that the 
  model is instantiated and the save\_model method is called.

  \item{test-id2\\}
  \textbf{Type:} Automatic \\
  \textbf{Initial State:} N/A \\
  \textbf{Input:} 2 to classify the image. \\
  \textbf{Output:} Pass if the model classifies input correctly, and Fail if it is not. \\
  \textbf{Test Case Derivation:} 
  \begin{itemize}
      \item The main function should instantiate an Output object, call its set\_class\_name 
      method, and then call its save\_feedback method when the user selects option '2' to 
      classify an image and provide feedback.
      \item Therefore, if the main function executes successfully, it indicates that the 
      image is classified correctly, and the set\_class\_name and save\_feedback methods are called, resulting in a Pass.
  \end{itemize}
  \textbf{How test will be performed:} 
  Run the main function with input '2' to simulate classifying an image and providing 
  feedback, and verify that the Output object is instantiated, its methods set\_class\_name and save\_feedback are called.
\end{enumerate}

\subsubsection{ Saved ANN Model Module (M3)}
This document outlines the unit tests for the `Model` class which handles the 
saving, loading, and processing of machine learning model parameters.

\begin{enumerate}

  \item{test-id3\\}
  
  \textbf{Type:} Automatic
  
  \textbf{Initial State:} N/A
  
  \textbf{Input:} Call to `load\_model` method with a non-existent file name.
  
  \textbf{Output:} Pass if a FileNotFoundError is raised, indicating correct 
  error handling; Fail otherwise.
  
  \textbf{Test Case Derivation:}
  \begin{itemize}
    \item The `load\_model` method is expected to raise a FileNotFoundError when it 
    fails to find the specified file, demonstrating the method's robustness in handling file read errors.
  \end{itemize}
  
  \textbf{How test will be performed:} 
  The test will attempt to load a non-existent file by calling the `load\_model` 
  method and check for a FileNotFoundError to confirm proper error handling.
  
  \item{test-id4\\}
  
  \textbf{Type:} Automatic
  
  \textbf{Initial State:} N/A
  
  \textbf{Input:} Successful data loading using the `load\_model` method.
  
  \textbf{Output:} Pass if the loaded data matches expected parameters; Fail otherwise.
  
  \textbf{Test Case Derivation:}
  \begin{itemize}
    \item This test checks if the `load\_model` method can correctly load data 
    from a file and return the appropriate data structure. A match with expected 
    parameters indicates correct functionality.
  \end{itemize}
  
  \textbf{How test will be performed:} 
  The test will load data using the `load\_model` method and verify that the 
  returned data matches predefined expected values.
  
  \item{test-id5\\}
  
  \textbf{Type:} Automatic
  
  \textbf{Initial State:} N/A
  
  \textbf{Input:} Invocation of the `save\_model` method under normal conditions.
  
  \textbf{Output:} Pass if the method successfully saves data and returns True; Fail otherwise.
  
  \textbf{Test Case Derivation:}
  \begin{itemize}
    \item The `save\_model` method should save the model parameters to a file and 
    return True upon successful completion. This test verifies that the method 
    performs as expected when no errors occur.
  \end{itemize}
  
  \textbf{How test will be performed:} 
  Run the `save\_model` method with typical parameters and check that it returns 
  True, indicating successful data saving.
  
  \item{test-id6\\}
  
  \textbf{Type:} Automatic
  
  \textbf{Initial State:} N/A
  
  \textbf{Input:} Attempt to save data using the `save\_model` method 
  when a PermissionError is expected.
  
  \textbf{Output:} Pass if a PermissionError is raised, confirming that the 
  method correctly handles permission issues; Fail otherwise.
  
  \textbf{Test Case Derivation:}
  \begin{itemize}
    \item The test aims to confirm that the `save\_model` method can 
    appropriately handle a PermissionError by raising the expected 
    exception when file writing permissions are restricted.
  \end{itemize}
  
  \textbf{How test will be performed:} 
  The test will simulate a permission error during the file-saving 
  process and verify that a PermissionError is raised.
  
  \item{test-id7\\}
  \textbf{Type:} Automatic\\
  \textbf{Initial State:} N/A\\
  \textbf{Input:} Parameters provided to `load\_trained\_classifier` to
   simulate classification.\\
  \textbf{Output:} Pass if the classifier correctly predicts using provided 
  parameters; Fail otherwise.\\
  \textbf{Test Case Derivation:}
  \begin{itemize}
      \item This test verifies that the `load\_trained\_classifier` method correctly 
      utilizes loaded model weights to predict and return the classification.
      \item The method should return 'TestClass' as the prediction output when 
      using predefined weights and input data.
  \end{itemize}
  \textbf{How test will be performed:}
  \begin{itemize}
      \item Run the method with mocked `load\_model` to return specific weights and 
      check if the predicted class matches 'TestClass'.
  \end{itemize}
  
  \item{test-id8\\}
  \textbf{Type:} Automatic\\
  \textbf{Initial State:} N/A\\
  \textbf{Input:} Simulated scenario where `load\_model` cannot read the file 
  (IOError).\\
  \textbf{Output:} Pass if IOError is raised correctly indicating file read 
  issues; Fail otherwise.\\
  \textbf{Test Case Derivation:}
  \begin{itemize}
      \item The `load\_model` method is expected to handle read errors gracefully by 
      raising an IOError if the file is not accessible or corrupt.
  \end{itemize}
  \textbf{How test will be performed:}
  \begin{itemize}
      \item Attempt to load a file with a mocked `numpy.load` setup to raise an 
      IOError and verify that the error is correctly raised and identified.
  \end{itemize}
  
  \item{test-id9\\}
  \textbf{Type:} Automatic\\
  \textbf{Initial State:} N/A\\
  \textbf{Input:} Testing `load\_model` method under conditions that trigger an 
  IOError (disk write failure).\\
  \textbf{Output:} Pass if IOError is correctly identified and reported; 
  Fail otherwise.\\
  \textbf{Test Case Derivation:}
  \begin{itemize}
      \item This test ensures that the `load\_model` method can appropriately 
      handle and report disk write errors through proper error messaging.
  \end{itemize}
  \textbf{How test will be performed:}
  \begin{itemize}
      \item Simulate a disk write error using a mocked `numpy.save` function 
      that raises an IOError and check for the correct error handling.
  \end{itemize}
  
  \item{test-id10\\}
  \textbf{Type:} Automatic\\
  \textbf{Initial State:} N/A\\
  \textbf{Input:} Scenario where `load\_model` faces an unexpected error 
  type (generic Exception).\\
  \textbf{Output:} Pass if the generic Exception is handled and re-reported 
  correctly; Fail otherwise.\\
  \textbf{Test Case Derivation:}
  \begin{itemize}
      \item To validate that `load\_model` properly re-raises exceptions
       with a custom message, ensuring that all potential errors are accounted for and clearly communicated.
  \end{itemize}
  \textbf{How test will be performed:}
  \begin{itemize}
      \item Induce a generic error in the `load\_model` method using a 
      mocked `numpy.save` and verify the exception handling by checking the re-raised error message.
  \end{itemize}
  
  \end{enumerate}
  
\subsection{ Output Module (M4)} 
This document details the unit tests for the `Output` class, which manages file operations 
sand user interactions for classifying images.

\begin{enumerate}

\item{test-id11\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} Text "Test sentence" to be appended to "feedback.txt".\\
\textbf{Output:} Pass if the text is appended correctly; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item This test verifies that the `append\_to\_file` method correctly 
    opens a file in append mode and writes the specified text followed by a newline.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
    \item The method is called with the specified inputs, and the mock 
    verifies that the file operations are performed correctly.
\end{itemize}

\item{test-id12\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} Attempt to append text to a non-existent file.\\
\textbf{Output:} Pass if a FileNotFoundError is raised; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item The method should raise a FileNotFoundError if it attempts to 
    open a non-existent file, indicating robust error handling.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
    \item The method is called, and the FileNotFoundError is expected to 
    be raised due to the mocked file operation.
\end{itemize}

\item{test-id13\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} Simulate an IOError during file writing.\\
\textbf{Output:} Pass if an IOError is correctly raised; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item The test checks for proper exception handling when an IOError 
    occurs due to file writing issues.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
    \item An IOError is simulated through mocking, and the method is 
    expected to raise this error appropriately.
\end{itemize}

\item{test-id14\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} Simulate an unexpected exception during file operation.\\
\textbf{Output:} Pass if the unexpected exception is handled and logged 
correctly; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item The method should log and handle any unexpected exceptions that 
    are not explicitly caught by earlier error handling code.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
    \item An unexpected exception (generic Exception) is simulated to test 
    the robustness of the method's error handling.
\end{itemize}
\item{test-id15\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} User inputs 'y' indicating agreement with the classification.\\
\textbf{Output:} Pass if feedback is saved correctly as "dog dog"; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item This test checks whether the `save\_feedback` method correctly handles 
    user agreement by appending the correct feedback to the file.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
    \item The test simulates user input of 'y' for agreement and verifies that 
    the feedback "dog dog" is correctly appended to the feedback file.
\end{itemize}

\item{test-id16\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} User inputs 'n' followed by 'cat', indicating disagreement and 
providing an alternative classification.\\
\textbf{Output:} Pass if feedback is saved correctly as "dog cat"; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item This test verifies that the `save\_feedback` method properly records a 
    user's disagreement and alternative class name by appending the correct feedback to the file.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
    \item The test simulates user inputs of 'n' and 'cat', and checks that the 
    feedback "dog cat" is appended to the feedback file.
\end{itemize}

\item{test-id17\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} User inputs 'n', followed by an invalid class name 'xyz', and then 'cat'.\\
\textbf{Output:} Pass if feedback is saved correctly as "dog cat" after user 
corrects the class name; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item This test assesses the `save\_feedback` method's ability to handle 
    multiple user inputs where the first provided class name is invalid, 
    ensuring that only valid feedback is recorded.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
    \item The test simulates initial disagreement with the classification, 
    an invalid class name input followed by a valid name, and checks the final written feedback.
\end{itemize}

\item{test-id18\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} Simulate user input that categorizes an image as 'cat'.\\
\textbf{Output:} Pass if the set\_class\_name method returns 'cat'; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item This test verifies the functionality of the `set\_class\_name` 
    method to correctly classify an image based on simulated input.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
    \item Mock methods are used to simulate processing an image and setting its 
    classification as 'cat'. The test checks that the method returns the correct classification.
\end{itemize}

\end{enumerate}

\subsubsection{Data Preparing and Preprocessing Module (M9)}
This document outlines the unit tests for the `Data` class, which is responsible 
for loading, processing, and preparing datasets for machine learning tasks.

\begin{enumerate}

\item{test-id19\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} Simulated CIFAR-10 dataset load operation.\\
\textbf{Output:} Pass if the dataset is loaded correctly with proper shapes; 
Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item This test verifies that the `load\_data` method can load data correctly and 
    validate its shape to ensure it meets expected dimensions.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
    \item Mocking is used to simulate the loading operation and shape checking is 
    performed to confirm data integrity.
\end{itemize}

\item{test-id20\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} Attempt to load CIFAR-10 data with incorrect shapes to simulate a 
failed operation.\\
\textbf{Output:} Pass if an appropriate exception is raised due to incorrect data 
shapes; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item This test aims to ensure that the `load\_data` method raises an 
    exception when data does not meet required specifications, maintaining robustness in data handling.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
    \item The loading function is mocked to return data with incorrect 
    shapes, and the method's error handling is tested.
\end{itemize}

\item{test-id21\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} RGB images are processed to convert them into grayscale 
using the method under test.\\
\textbf{Output:} Pass if all images are correctly converted to grayscale; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item The `rgb2gray` method should accurately convert RGB images to grayscale 
    by applying the correct formula. This test checks the method's effectiveness across multiple images.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
    \item A batch of RGB images is processed, and the output is verified 
    against expected grayscale values.
\end{itemize}

\item{test-id22\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} Normalization of pixel values in images from 0-255 to 0-1 range.\\
\textbf{Output:} Pass if pixel values are correctly normalized; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item The `prep\_pixels` method is tested to ensure it can normalize image data 
    correctly, crucial for machine learning model input preparation.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
    \item The normalization process is applied to a set of images, and the results 
    are checked to confirm they fall within the expected range.
\end{itemize}

\item{test-id23\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} Flattening 2D image data into 1D vectors.\\
\textbf{Output:} Pass if images are correctly flattened without 
data loss; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
    \item Ensuring that the `flat\_data` method effectively reshapes 
    2D images into 1D format without altering the data integrity is crucial 
    for input to certain types of neural networks.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
  \item Images are converted from 2D to 1D using the flattening process, 
  and the output dimensions are checked to verify correctness.
\end{itemize}

\item{test-id24\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} Shuffling of labeled image data to ensure randomness in data order.\\
\textbf{Output:} Pass if data is correctly shuffled and labels are maintained; Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
  \item The `shuffle\_data` method is essential for randomizing the order of image 
  data to prevent model bias during training. This test checks that the shuffling maintains correct 
  label associations and achieves randomness.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
  \item The dataset is shuffled using the method, and checks are made to ensure 
  that no data is lost or mismatched during the process, verifying that the order of data is altered.
\end{itemize}

\item{test-id25\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} Grayscale conversion for a single RGB image to validate the 
method's precision on individual images.\\
\textbf{Output:} Pass if the single image is converted accurately to grayscale; 
Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
  \item Single image testing isolates the performance of the `rgb2gray` method to ensure 
  it can accurately apply the grayscale formula on an individual basis.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
  \item A single RGB image is processed, and the resulting grayscale image is compared 
  against expected values calculated using the standard luminosity formula.
\end{itemize}

\item{test-id26\\}
\textbf{Type:} Automatic\\
\textbf{Initial State:} N/A\\
\textbf{Input:} Conversion of a batch of RGB images to grayscale to check consistent 
application across multiple instances.\\
\textbf{Output:} Pass if all images in the batch are consistently converted to grayscale; 
Fail otherwise.\\
\textbf{Test Case Derivation:}
\begin{itemize}
  \item This test evaluates the consistency and reliability of the `rgb2gray` method when 
  applied to multiple images, ensuring that the conversion formula is uniformly applied.
\end{itemize}
\textbf{How test will be performed:}
\begin{itemize}
  \item A batch of images is processed, and each converted image is checked against 
  expected grayscale values to confirm uniform application of the conversion formula.
\end{itemize}

\end{enumerate}



\subsection{Tests for Nonfunctional Requirements}

Unit testing the non-functional requirements is beyond the scope.

\subsection{Traceability Between Test Cases and Modules}

A traceability between test cases and modules is shown in \autoref{tab:tc-traceability-module}.

\begin{table}[h!]
\begin{center}
\begin{tabular}{ l|c|c|c|c|c|c|c|c|c }
\hline
 & M2 & M3 & M4 & M5 & M6 & M7 & M8 & M9 & M10\\
\hline
T1 & & & & & X & & & &\\
\hline
T2 & & X & X & X & & & & & X\\
\hline
test-id1 - test-id2 & X & & & & & & & &\\
\hline
test-id3 - test-id10 & & X & & & & X & & & X\\
\hline
test-id11 - test-id18 & & & X & X & & X & & &\\
\hline
test-id19 - test-id26 & & & & & & & X & X &\\

\hline
\end{tabular}
\caption{Tracebility between test cases and module}
\label{tab:tc-traceability-module}
\end{center}
\end{table}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

% \section{Appendix}

% This is where you can place additional information.

% \subsection{Symbolic Parameters}

% The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
% Their values are defined in this section for easy maintenance.

% \subsection{Usability Survey Questions?}

% \wss{This is a section that would be appropriate for some projects.}

% \newpage{}
% \section*{Appendix --- Reflection}

% The information in this section will be used to evaluate the team members on the
% graduate attribute of Lifelong Learning.  Please answer the following questions:

% \newpage{}
% \section*{Appendix --- Reflection}

% \wss{This section is not required for CAS 741}

% The information in this section will be used to evaluate the team members on the
% graduate attribute of Lifelong Learning.  Please answer the following questions:

% \begin{enumerate}
%   \item What knowledge and skills will the team collectively need to acquire to
%   successfully complete the verification and validation of your project?
%   Examples of possible knowledge and skills include dynamic testing knowledge,
%   static testing knowledge, specific tool usage etc.  You should look to
%   identify at least one item for each team member.
%   \item For each of the knowledge areas and skills identified in the previous
%   question, what are at least two approaches to acquiring the knowledge or
%   mastering the skill?  Of the identified approaches, which will each team
%   member pursue, and why did they make this choice?
% \end{enumerate}

\end{document}